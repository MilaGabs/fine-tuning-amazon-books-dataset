{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4147,
     "status": "ok",
     "timestamp": 1734093481288,
     "user": {
      "displayName": "João Gabriel",
      "userId": "02887411909052297991"
     },
     "user_tz": 180
    },
    "id": "Q9_Ad_fb8ZQU",
    "outputId": "a4f27653-a573-4f93-8356-4af5eba6faff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1456,
     "status": "ok",
     "timestamp": 1734093485525,
     "user": {
      "displayName": "João Gabriel",
      "userId": "02887411909052297991"
     },
     "user_tz": 180
    },
    "id": "l0t91yL-5jNk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jsonlines\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1710,
     "status": "ok",
     "timestamp": 1734093488500,
     "user": {
      "displayName": "João Gabriel",
      "userId": "02887411909052297991"
     },
     "user_tz": 180
    },
    "id": "QnikIXgX8b_H",
    "outputId": "fb282a5e-33c9-4d57-951b-90bb2bcfbb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/content/drive/MyDrive/Colab Notebooks/dataset/trn.json\"\n",
    "cleaned_output_file = \"/content/drive/MyDrive/Colab Notebooks/dataset/cleaned_dataset.json\"\n",
    "transformed_output_file = \"/content/drive/MyDrive/Colab Notebooks/dataset/transformed_dataset.json\"\n",
    "fraction = 0.6\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RFcFHr-m-Lxe"
   },
   "outputs": [],
   "source": [
    "def load_dataset(input_file: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Carrega o dataset a partir de um arquivo JSONLines.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Caminho do arquivo JSONLines de entrada.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Lista de dicionários com os dados do JSONLines.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with jsonlines.open(input_file, mode='r') as reader:\n",
    "            return [line for line in reader]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erro ao carregar o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_inputs(input_file: str, fraction: float) -> None:\n",
    "    \"\"\"\n",
    "    Valida os inputs para a função create_sample_dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"O arquivo {input_file} não foi encontrado.\")\n",
    "    if not (0 < fraction <= 1):\n",
    "        raise ValueError(\"A fração deve estar no intervalo (0, 1].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wFF8W9ck-YF0"
   },
   "outputs": [],
   "source": [
    "def filter_empty_entries(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove entradas vazias do DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame contendo as colunas 'title' e 'content'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame filtrado sem entradas vazias.\n",
    "    \"\"\"\n",
    "    print(f\"Linhas antes da filtragem: {len(data)}\")\n",
    "    filtered_data = data.dropna(subset=['title', 'content']).query(\"title != '' and content != ''\")\n",
    "    print(f\"Linhas após a filtragem: {len(filtered_data)}\")\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_entities = {\n",
    "    \"&quot;\": '\"',    # Aspas duplas\n",
    "    \"&apos;\": \"'\",    # Aspas simples/apóstrofo\n",
    "    \"&amp;\": \"&\",     # E comercial\n",
    "    \"&lt;\": \"<\",      # Menor que\n",
    "    \"&gt;\": \">\",      # Maior que\n",
    "    \"&nbsp;\": \" \",    # Espaço não separável\n",
    "    \"&ldquo;\": \"“\",   # Aspas duplas esquerda\n",
    "    \"&rdquo;\": \"”\",   # Aspas duplas direita\n",
    "    \"&lsquo;\": \"‘\",   # Aspas simples esquerda\n",
    "    \"&rsquo;\": \"’\",   # Aspas simples direita\n",
    "    \"&copy;\": \"©\",    # Símbolo de copyright\n",
    "    \"&ndash;\": \"–\",   # Meia-risca\n",
    "    \"&mdash;\": \"—\",   # Travessão\n",
    "    \"&x201C\": \"“\",    # Aspas duplas esquerda\n",
    "    \"&x201D\": \"”\",    # Aspas duplas direita\n",
    "    \"&x2019\": \"’\",    # Apóstrofo tipográfico\n",
    "    \"&hellip;\": \"…\",  # Reticências\n",
    "}\n",
    "def remove_html_entities(text, entity_map):\n",
    "    \"\"\"\n",
    "    Limpa o texto, removendo caracteres especiais.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto que a ser limpo;\n",
    "        entity_map: Mapa de entidades a serem removidas do texto\n",
    "    Returns:\n",
    "        O texto limpo em formato string.\n",
    "    \"\"\"\n",
    "\n",
    "    for entity in entity_map.keys():\n",
    "        text = text.replace(entity, \"\")\n",
    "\n",
    "    text = re.sub(r\"&x[0-9A-Fa-f]+;?\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WFnPHKGb-b68"
   },
   "outputs": [],
   "source": [
    "def create_sample_dataset(input_file: str, output_file: str, fraction: float, seed: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Extrai uma fração aleatória do dataset, mantém 'title' e 'content',\n",
    "    remove entradas vazias e salva em um novo arquivo.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Caminho do arquivo JSONLines de entrada.\n",
    "        output_file (str): Caminho do arquivo JSONLines de saída.\n",
    "        fraction (float): Fração do dataset a ser extraída.\n",
    "        seed (int): Semente para garantir reprodutibilidade.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int]: Total de linhas no dataset final e total de registros originais.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        validate_inputs(input_file, fraction)\n",
    "\n",
    "        print(\"Carregando o dataset...\")\n",
    "        data = load_dataset(input_file)\n",
    "\n",
    "        if not data:\n",
    "            print(\"O arquivo de entrada está vazio.\")\n",
    "            return 0, 0\n",
    "\n",
    "        random.seed(seed)\n",
    "\n",
    "        total_samples = len(data)\n",
    "        num_samples = max(1, int(total_samples * fraction))\n",
    "        print(f\"Total de registros no arquivo original: {total_samples}\")\n",
    "        print(f\"Número de registros selecionados: {num_samples}\")\n",
    "        sampled_data = random.sample(data, num_samples)\n",
    "\n",
    "        print(\"Processando os dados...\")\n",
    "        mapa = str.maketrans(\"\", \"\", \"@#*%$;[]'/\")\n",
    "        processed_data = [\n",
    "            {\"title\": remove_html_entities(entry.get(\"title\", \"\").translate(mapa).replace(\"&quot\", '').replace('\\\"', '') , html_entities), \"content\": remove_html_entities(entry.get(\"content\", \"\").translate(mapa).replace(\"&quot\", '').replace('\\\"', '') , html_entities) }\n",
    "            for entry in sampled_data\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        df = filter_empty_entries(df)\n",
    "\n",
    "        print(f\"Salvando o dataset em {output_file}...\")\n",
    "        df.to_json(output_file, orient=\"records\", lines=True, force_ascii=False)\n",
    "        print(f\"Dataset tratado salvo com sucesso em: {output_file}\")\n",
    "\n",
    "        total_final = len(df)\n",
    "        return total_final, total_samples, df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o processamento: {e}\", exc_info=True)\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "grxDeHT0-e_Q"
   },
   "outputs": [],
   "source": [
    "def analyze_dataset(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Analisa um arquivo JSONLines para verificar se todas as linhas seguem o formato esperado.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo JSONLines.\n",
    "    \"\"\"\n",
    "    total_lines = 0\n",
    "    valid_lines = 0\n",
    "    missing_title = 0\n",
    "    missing_content = 0\n",
    "    empty_title = 0\n",
    "    empty_content = 0\n",
    "\n",
    "    try:\n",
    "        print(f\"Analisando o arquivo: {file_path}\")\n",
    "        with jsonlines.open(file_path, mode='r') as reader:\n",
    "            for line in reader:\n",
    "                total_lines += 1\n",
    "\n",
    "                has_title = \"title\" in line\n",
    "                has_content = \"content\" in line\n",
    "\n",
    "                if has_title and has_content:\n",
    "                    valid_lines += 1\n",
    "\n",
    "                    if not line[\"title\"].strip():\n",
    "                        empty_title += 1\n",
    "                    if not line[\"content\"].strip():\n",
    "                        empty_content += 1\n",
    "                else:\n",
    "                    if not has_title:\n",
    "                        missing_title += 1\n",
    "                    if not has_content:\n",
    "                        missing_content += 1\n",
    "\n",
    "        # Exibir resumo\n",
    "        print(\"Resumo da análise do dataset:\")\n",
    "        print(f\"Total de linhas: {total_lines}\")\n",
    "        print(f\"Linhas válidas: {valid_lines}\")\n",
    "        print(f\"Linhas com 'title' ausente: {missing_title}\")\n",
    "        print(f\"Linhas com 'content' ausente: {missing_content}\")\n",
    "        print(f\"Linhas com 'title' vazio: {empty_title}\")\n",
    "        print(f\"Linhas com 'content' vazio: {empty_content}\")\n",
    "        print(f\"Linhas inválidas: {total_lines - valid_lines}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao analisar o dataset: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(df, output_file):\n",
    "    \"\"\"\n",
    "    Formata o dataset para o formato que o modelo esta esperando para ser treinado.\n",
    "\n",
    "    Args:\n",
    "        df: Dataset ja processado e limpo.\n",
    "        output_file: Caminho onde o arquivo transformado será salvo\n",
    "    \"\"\"\n",
    "    transformed_data = {\n",
    "        \"instruction\": [\"SUMMARIZE THIS PRODUCTS\"] * len(df),\n",
    "        \"input\": df[\"title\"].tolist(),\n",
    "        \"output\": df[\"content\"].tolist()\n",
    "    }\n",
    "\n",
    "    # Salvar o novo dataset em um arquivo JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(transformed_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f35861Ic-hg_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisando o arquivo: C:/Projects/FIAP_IA_Fro_Devs/TechChallenge-Fase3/Tests/LF-Amazon/trn-folder/trn.json\n",
      "Resumo da análise do dataset:\n",
      "Total de linhas: 2248619\n",
      "Linhas válidas: 2248619\n",
      "Linhas com 'title' ausente: 0\n",
      "Linhas com 'content' ausente: 0\n",
      "Linhas com 'title' vazio: 126834\n",
      "Linhas com 'content' vazio: 749901\n",
      "Linhas inválidas: 0\n",
      "Iniciando o processamento do dataset...\n",
      "Carregando o dataset...\n",
      "Total de registros no arquivo original: 2248619\n",
      "Número de registros selecionados: 1349171\n",
      "Processando os dados...\n",
      "Linhas antes da filtragem: 1349171\n",
      "Linhas após a filtragem: 834306\n",
      "Salvando o dataset em C:/Projects/FIAP_IA_Fro_Devs/TechChallenge-Fase3/Tests/LF-Amazon/trn-folder/cleaned_dataset.json...\n",
      "Dataset tratado salvo com sucesso em: C:/Projects/FIAP_IA_Fro_Devs/TechChallenge-Fase3/Tests/LF-Amazon/trn-folder/cleaned_dataset.json\n",
      "Processamento concluído. Total de registros no dataset final: 834306 (37.10% do total original).\n",
      "Analisando o arquivo: C:/Projects/FIAP_IA_Fro_Devs/TechChallenge-Fase3/Tests/LF-Amazon/trn-folder/cleaned_dataset.json\n",
      "Resumo da análise do dataset:\n",
      "Total de linhas: 834306\n",
      "Linhas válidas: 834306\n",
      "Linhas com 'title' ausente: 0\n",
      "Linhas com 'content' ausente: 0\n",
      "Linhas com 'title' vazio: 0\n",
      "Linhas com 'content' vazio: 1\n",
      "Linhas inválidas: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(input_file)\n",
    "\n",
    "print(\"Iniciando o processamento do dataset...\")\n",
    "total_final, total_original, df = create_sample_dataset(input_file, cleaned_output_file, fraction, seed)\n",
    "\n",
    "if total_original > 0:\n",
    "    final_percentage = (total_final / total_original) * 100\n",
    "    print(\n",
    "        f\"Processamento concluído. Total de registros no dataset final: {total_final} \"\n",
    "        f\"({final_percentage:.2f}% do total original).\"\n",
    "    )\n",
    "else:\n",
    "    print(\"O dataset original estava vazio ou houve um erro no processamento.\")\n",
    "\n",
    "analyze_dataset(cleaned_output_file)\n",
    "\n",
    "transform_dataset(df, transformed_output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuwokZwXsSfv2ZNmkbRr4t",
   "mount_file_id": "1-Lher_lUMqby9LOGtRoczaQp7c3wI7Tk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
